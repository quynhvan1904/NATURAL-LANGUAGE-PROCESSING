# -*- coding: utf-8 -*-
"""NLP | Source code.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/15Z5jS-Of3eMimWO2KkxI1czXx0tLLsEe

link github: https://github.com/kimvo646/NLP.git

# Cài đặt

## pip install
"""

"""## Thư viện"""

# Xử lý dữ liệu
import pandas as pd
import numpy as np
import string
import re
import nltk
from underthesea import sent_tokenize, word_tokenize
from itertools import chain
from collections import Counter
from joblib import load

# Trực quan hóa
import matplotlib.pyplot as plt
import seaborn as sns
from matplotlib.ticker import MaxNLocator
from wordcloud import WordCloud

# Mô hình
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score, confusion_matrix

## LSTM
import gensim
from gensim.models import Word2Vec
from tensorflow.keras.utils import to_categorical
from keras.models import Sequential
from keras.layers import LSTM, Dense, Dropout
from keras.callbacks import EarlyStopping
from tensorflow.keras.models import load_model
from joblib import dump

# Khác
import warnings
warnings.filterwarnings("ignore")

"""## input"""

link_train = 'https://raw.githubusercontent.com/kimvo646/NLP/main/Web_demo/Data_csv/train_data.csv'
train = pd.read_csv(link_train, sep='\t', encoding='utf-16')
train.head(10)

link_test = 'https://raw.githubusercontent.com/kimvo646/NLP/main/Web_demo/Data_csv/test_data.csv'
test = pd.read_csv(link_test, sep='\t', encoding='utf-16')
test.head()

link_valid = 'https://raw.githubusercontent.com/kimvo646/NLP/main/Web_demo/Data_csv/validation_data.csv'
valid = pd.read_csv(link_valid, sep='\t', encoding='utf-16')
valid.head()

"""# Tổng quan bộ dữ liệu"""

# Tổng số dòng, số cột của bộ dữ liệu
print ('Các cột hiện có của bộ dữ liệu:')
for x in train.columns:
  print('>', x)
print(f"Bộ dữ liệu bao gồm {train.shape[1]} cột và {train.shape[0]} dòng")

"""# Tiền xử lý

## Kiểm tra các giá trị bị thiếu
"""

train.info()

train.isna().sum()

"""## Tách từ"""

def normalize_text(s):
    # Uncased
    s = s.lower()

    # Remove punctuations
    s = ''.join(ch for ch in s if ch not in string.punctuation)

    # Remove entities
    s = re.sub(r'\b((\w+|)wzjwz\d+)\b', " ", s)

    # Remove numbers
    s = re.sub(r'\d', ' ', s)

     # Fix whitespaces
    s = re.sub(r'\s+', ' ', s)

    #Remove leading and trailing spaces
    s = s.strip()

    return s

def tokenizer(text):
    tokens = []
    for sent in sent_tokenize(text):
        words = word_tokenize(sent)
        tokens.extend(words)
    return tokens

"""##Text mining"""

train_x=train['sentence'].tolist()
train_x=[normalize_text(sentence)for sentence in train_x]
all_tokens_train = [tokenizer(sentence) for sentence in train_x]

valid_x = valid['sentence'].tolist()
valid_x=[normalize_text(sentence)for sentence in valid_x]
all_tokens_valid = [tokenizer(sentence) for sentence in valid_x]

test_x = test['sentence'].tolist()
test_x=[normalize_text(sentence)for sentence in test_x]
all_tokens_test = [tokenizer(sentence) for sentence in test_x]


"""## Word2Vec
Nguồn tham khảo W2v: https://github.com/namlv97/biLSTM-vietnamese-uit-student-feedbacks/blob/main/biLSTM_Vietnamese_uit_student_feedbacks.ipynb
"""

# Cài đặt các chỉ số
min_count=1
window=3
vector_size=300
alpha=1e-3
min_alpha=1e-4
negative=10

word_sents_train=[sent for sent in all_tokens_train]
# Tạo mô hình Word2Vec
w2v_model = Word2Vec(min_count=min_count, window=window, vector_size=vector_size, alpha=alpha, min_alpha=min_alpha, negative=negative, sg=1)
# Xây dựng từ điển cho tập dữ liệu
w2v_model.build_vocab(word_sents_train)
#Huấn luyện mô hình
w2v_model.train(word_sents_train, total_examples=w2v_model.corpus_count, epochs=100, report_delay=1,compute_loss=True)

# Huấn luyện cho tập all_tokens_test
word_sents_test = [sent for sent in all_tokens_test]
w2v_model.build_vocab(word_sents_test, update=True)
w2v_model.train(word_sents_test, total_examples=w2v_model.corpus_count, epochs=100, report_delay=1, compute_loss=True)

# Huấn luyện cho tập all_tokens_valid
word_sents_valid = [sent for sent in all_tokens_valid]
w2v_model.build_vocab(word_sents_valid, update=True)
w2v_model.train(word_sents_valid, total_examples=w2v_model.corpus_count, epochs=100, report_delay=1, compute_loss=True)

# Lưu mô hình sau khi huấn luyện
w2v_model.save("model_w2v")

"""# Huấn luyện mô hình

"""
##Logistic Regression

### Topic
def get_sentence_vector(sentence, model):
    vector_sum = np.zeros(model.vector_size)
    count = 0
    for word in sentence:
        if word in model.wv:
            vector_sum += model.wv[word]
            count += 1
    if count != 0:
        return vector_sum / count
    else:
        return vector_sum

X_train_w2v = [get_sentence_vector(sentence, w2v_model) for sentence in all_tokens_train]
X_test_w2v = [get_sentence_vector(sentence, w2v_model) for sentence in all_tokens_test]

# Chia dữ liệu thành features và labels
X_train_w2v = np.array(X_train_w2v)
X_test_w2v = np.array(X_test_w2v)

y_train_topic = train['topic']
y_test_topic = test['topic']

logistic_model_topic = LogisticRegression(multi_class='multinomial', solver='lbfgs', max_iter=100, random_state=42)
logistic_model_topic.fit(X_train_w2v, train['topic'])
pred_topic = logistic_model_topic.predict(X_test_w2v)

dump(logistic_model_topic, 'model_Logistic_Regression_topic.joblib')

accuracy = accuracy_score(y_test_topic, pred_topic)
precision = precision_score(y_test_topic, pred_topic, average='weighted')
recall = recall_score(y_test_topic, pred_topic, average='weighted')
f1 = f1_score(y_test_topic, pred_topic, average='weighted')
print(f'Accuracy: {accuracy}')
print(f'Precision: {precision}')
print(f'Recall: {recall}')
print(f'F1 Score: {f1}')


"""### Sentiment"""

X_train_w2v = [get_sentence_vector(sentence, w2v_model) for sentence in all_tokens_train]
X_test_w2v = [get_sentence_vector(sentence, w2v_model) for sentence in all_tokens_test]

# Chia dữ liệu thành features và labels
X_train_w2v = np.array(X_train_w2v)
X_test_w2v = np.array(X_test_w2v)
# Chia dữ liệu thành features và labels cho topic
y_train_sentiment = train['sentiment']
y_test_sentiment = test['sentiment']

logistic_model_sentiment = LogisticRegression(multi_class='multinomial', solver='lbfgs', max_iter=100, random_state=42)
logistic_model_sentiment.fit(X_train_w2v, train['sentiment'])
pred_sentiment = logistic_model_sentiment.predict(X_test_w2v)

dump(logistic_model_sentiment, 'model_Logistic_Regression_sentiment.joblib')

accuracy = accuracy_score(y_test_sentiment, pred_sentiment)
precision = precision_score(y_test_sentiment, pred_sentiment, average='weighted')
recall = recall_score(y_test_sentiment, pred_sentiment, average='weighted')
f1 = f1_score(y_test_sentiment, pred_sentiment, average='weighted')
print(f'Accuracy: {accuracy}')
print(f'Precision: {precision}')
print(f'Recall: {recall}')
print(f'F1 Score: {f1}')



