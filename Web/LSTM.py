# -*- coding: utf-8 -*-
"""NLP | Source code.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/15Z5jS-Of3eMimWO2KkxI1czXx0tLLsEe

link github: https://github.com/kimvo646/NLP.git

# Cài đặt

## pip install
"""

"""## Thư viện"""

# Xử lý dữ liệu
import pandas as pd
import numpy as np
import string
import re
import nltk
from underthesea import sent_tokenize, word_tokenize
from itertools import chain
from collections import Counter
from joblib import load

# Trực quan hóa
import matplotlib.pyplot as plt
import seaborn as sns
from matplotlib.ticker import MaxNLocator
from wordcloud import WordCloud

# Mô hình
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score, confusion_matrix

## LSTM
import gensim
from gensim.models import Word2Vec
from tensorflow.keras.utils import to_categorical
from keras.models import Sequential
from keras.layers import LSTM, Dense, Dropout
from keras.callbacks import EarlyStopping
from tensorflow.keras.models import load_model
from joblib import dump
import joblib

# Khác
import warnings
warnings.filterwarnings("ignore")

"""## input"""

link_train = 'https://raw.githubusercontent.com/kimvo646/NLP/main/Web_demo/Data_csv/train_data.csv'
train = pd.read_csv(link_train, sep='\t', encoding='utf-16')
train.head(10)

link_test = 'https://raw.githubusercontent.com/kimvo646/NLP/main/Web_demo/Data_csv/test_data.csv'
test = pd.read_csv(link_test, sep='\t', encoding='utf-16')
test.head()

link_valid = 'https://raw.githubusercontent.com/kimvo646/NLP/main/Web_demo/Data_csv/validation_data.csv'
valid = pd.read_csv(link_valid, sep='\t', encoding='utf-16')
valid.head()

"""# Tổng quan bộ dữ liệu"""

# Tổng số dòng, số cột của bộ dữ liệu
print ('Các cột hiện có của bộ dữ liệu:')
for x in train.columns:
  print('>', x)
print(f"Bộ dữ liệu bao gồm {train.shape[1]} cột và {train.shape[0]} dòng")

"""# Tiền xử lý

## Kiểm tra các giá trị bị thiếu
"""

train.info()

train.isna().sum()

"""## Tách từ"""

def normalize_text(s):
    # Uncased
    s = s.lower()

    # Remove punctuations
    s = ''.join(ch for ch in s if ch not in string.punctuation)

    # Remove entities
    s = re.sub(r'\b((\w+|)wzjwz\d+)\b', " ", s)

    # Remove numbers
    s = re.sub(r'\d', ' ', s)

     # Fix whitespaces
    s = re.sub(r'\s+', ' ', s)

    #Remove leading and trailing spaces
    s = s.strip()

    return s

def tokenizer(text):
    tokens = []
    for sent in sent_tokenize(text):
        words = word_tokenize(sent)
        tokens.extend(words)
    return tokens

"""##Text mining"""

train_x=train['sentence'].tolist()
train_x=[normalize_text(sentence)for sentence in train_x]
all_tokens_train = [tokenizer(sentence) for sentence in train_x]

valid_x = valid['sentence'].tolist()
valid_x=[normalize_text(sentence)for sentence in valid_x]
all_tokens_valid = [tokenizer(sentence) for sentence in valid_x]

test_x = test['sentence'].tolist()
test_x=[normalize_text(sentence)for sentence in test_x]
all_tokens_test = [tokenizer(sentence) for sentence in test_x]


"""## Word2Vec
Nguồn tham khảo W2v: https://github.com/namlv97/biLSTM-vietnamese-uit-student-feedbacks/blob/main/biLSTM_Vietnamese_uit_student_feedbacks.ipynb
"""

# Cài đặt các chỉ số
min_count=1
window=3
vector_size=300
alpha=1e-3
min_alpha=1e-4
negative=10

word_sents_train=[sent for sent in all_tokens_train]
# Tạo mô hình Word2Vec
w2v_model = Word2Vec(min_count=min_count, window=window, vector_size=vector_size, alpha=alpha, min_alpha=min_alpha, negative=negative, sg=1)
# Xây dựng từ điển cho tập dữ liệu
w2v_model.build_vocab(word_sents_train)
#Huấn luyện mô hình
w2v_model.train(word_sents_train, total_examples=w2v_model.corpus_count, epochs=100, report_delay=1,compute_loss=True)

# Huấn luyện cho tập all_tokens_test
word_sents_test = [sent for sent in all_tokens_test]
w2v_model.build_vocab(word_sents_test, update=True)
w2v_model.train(word_sents_test, total_examples=w2v_model.corpus_count, epochs=100, report_delay=1, compute_loss=True)

# Huấn luyện cho tập all_tokens_valid
word_sents_valid = [sent for sent in all_tokens_valid]
w2v_model.build_vocab(word_sents_valid, update=True)
w2v_model.train(word_sents_valid, total_examples=w2v_model.corpus_count, epochs=100, report_delay=1, compute_loss=True)

# Lưu mô hình sau khi huấn luyện
# w2v_model.save("model_w2v")
joblib.dump(w2v_model, "model_w2v.pkl")

"""# Huấn luyện mô hình
"""
"""## LSTM

### Topic
"""

# # Kích thước của từ điển
# vocab_size = len(w2v_model.wv.key_to_index) + 1

# Kích thước của vector nhúng
embedding_dim = w2v_model.vector_size

# Xây dựng mô hình LSTM
model_LSTM_topic = Sequential()
model_LSTM_topic.add(LSTM(units=100, return_sequences=True, input_shape=(1, embedding_dim)))
model_LSTM_topic.add(Dropout(0.2))
model_LSTM_topic.add(LSTM(units=50, return_sequences=True))
model_LSTM_topic.add(Dropout(0.2))
model_LSTM_topic.add(LSTM(units=50, return_sequences=True))
model_LSTM_topic.add(Dropout(0.2))
model_LSTM_topic.add(LSTM(units=50))
model_LSTM_topic.add(Dropout(0.2))
model_LSTM_topic.add(Dense(units=4, activation="softmax"))

# Biên dịch mô hình
model_LSTM_topic.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

print(model_LSTM_topic.summary())

def get_vector(word_list, model):
    # Khởi tạo một vector 0
    vec = np.zeros(model.vector_size).reshape((1, model.vector_size))
    count = 0.
    for word in word_list:
        # Thêm vector của từ vào vec
        vec += model.wv.get_vector(word).reshape((1, model.vector_size))
        count += 1.
    if count != 0:
        vec /= count
    return vec

X_train = np.concatenate([get_vector(sent, w2v_model) for sent in all_tokens_train])
X_valid = np.concatenate([get_vector(sent, w2v_model) for sent in all_tokens_valid])

#xác định các tập để huấn luyện mô hình
y_train = to_categorical(train['topic'])

X_train = X_train.reshape(-1, 1, embedding_dim)
X_valid = X_valid.reshape(-1, 1, embedding_dim)

# Huấn luyện mô hình
early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)
model_LSTM_topic.fit(X_train, y_train, epochs=100, batch_size=32, validation_data=(X_valid, to_categorical(valid['topic'])), callbacks=[early_stopping])

model_LSTM_topic.save('model_LSTM_topic.h5')

# Tạo vector nhúng cho dữ liệu kiểm tra
X_test = np.concatenate([get_vector(sent, w2v_model) for sent in all_tokens_test])
X_test = X_test.reshape(-1, 1, embedding_dim)

# Dự đoán nhãn cho dữ liệu kiểm tra
y_pred = model_LSTM_topic.predict(X_test)
y_pred = np.argmax(y_pred, axis=1)  # Chuyển từ dạng one-hot về dạng nhãn

# Chuyển nhãn thực tế sang dạng số
y_true = test['topic'].values

# Tính toán các chỉ số
accuracy = accuracy_score(y_true, y_pred)
recall = recall_score(y_true, y_pred, average='macro')
precision = precision_score(y_true, y_pred, average='macro')
f1 = f1_score(y_true, y_pred, average='macro')

print(f"Accuracy: {accuracy}")
print(f"Recall: {recall}")
print(f"Precision: {precision}")
print(f"F1 Score: {f1}")



"""### Sentiment"""

# Xây dựng mô hình LSTM
model_LSTM_sentiment = Sequential()
model_LSTM_sentiment.add(LSTM(units=100, return_sequences=True, input_shape=(1, embedding_dim)))
model_LSTM_sentiment.add(Dropout(0.2))
model_LSTM_sentiment.add(LSTM(units=50, return_sequences=True))
model_LSTM_sentiment.add(Dropout(0.2))
model_LSTM_sentiment.add(LSTM(units=50, return_sequences=True))
model_LSTM_sentiment.add(Dropout(0.2))
model_LSTM_sentiment.add(LSTM(units=50))
model_LSTM_sentiment.add(Dropout(0.2))
model_LSTM_sentiment.add(Dense(units=2, activation="softmax"))

# Biên dịch mô hình
model_LSTM_sentiment.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

print(model_LSTM_sentiment.summary())

#xác định các tập để huấn luyện mô hình
y_train = to_categorical(train['sentiment'])

# Huấn luyện mô hình
early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)
model_LSTM_sentiment.fit(X_train, y_train, epochs=100, batch_size=32, validation_data=(X_valid, to_categorical(valid['sentiment'])), callbacks=[early_stopping])

model_LSTM_sentiment.save('model_LSTM_sentiment.h5')

# Tạo vector nhúng cho dữ liệu kiểm tra
X_test = np.concatenate([get_vector(sent, w2v_model) for sent in all_tokens_test])
X_test = X_test.reshape(-1, 1, embedding_dim)

# Dự đoán nhãn cho dữ liệu kiểm tra
y_pred = model_LSTM_sentiment.predict(X_test)
y_pred = np.argmax(y_pred, axis=1)  # Chuyển từ dạng one-hot về dạng nhãn

# Chuyển nhãn thực tế sang dạng số
y_true = test['sentiment'].values

# Tính toán các chỉ số
accuracy = accuracy_score(y_true, y_pred)
recall = recall_score(y_true, y_pred, average='macro')
precision = precision_score(y_true, y_pred, average='macro')
f1 = f1_score(y_true, y_pred, average='macro')

print(f"Accuracy: {accuracy}")
print(f"Recall: {recall}")
print(f"Precision: {precision}")
print(f"F1 Score: {f1}")

